---
title: "TPFinal BCRA"
format: html
editor: visual
---

CASO Introduccion: ESTUDIO DE LAS DEUDAS REGISTRADAS DEL SISTEMA FINANCIERO ARGENTINO El Banco Central de la Rep√∫blica Argentina (BCRA) publica mensualmente un informe consolidado de deudas actuales e hist√≥ricas (24 meses), denominado 'Central de Deudores del Sistema Financiero' elaborado en funci√≥n de los datos recibidos de distintos tipos de entidades financieras (entidades financieras, empresas no financieras emisoras de tarjetas, fideicomisos financieros, otros proveedores no financieros de cr√©dito, etc.), las cuales deben obligatoriamente remitir mensualmente al BCRA, detallando la totalidad de las financiaciones con la correspondiente situaci√≥n de cada deudor. Cada deuda informada al BCRA es acompa√±ada de su situaci√≥n que es una aproximaci√≥n a la cantidad de d√≠as de atraso en el cumplimiento de pago: Situaci√≥n 1 \| Situaci√≥n normal: atraso en el pago que no supere los 31 d√≠as. Situaci√≥n 2 \| Riesgo bajo: atraso en el pago de m√°s de 31 d√≠as y hasta 90 d√≠as. Situaci√≥n 3 \| Riesgo miedo: atraso en el pago de m√°s de 90 d√≠as y hasta 180 d√≠as. Situaci√≥n 4 \| Riesgo alto: atraso en el pago de m√°s de 180 d√≠as a un a√±o. Situaci√≥n 5 \| Irrecuperable: atraso superior a un a√±o. Situaci√≥n 6 \| Irrecuperable por disposici√≥n t√©cnica: deuda con una ex entidad. Se cuenta con una muestra aleatoria de 19.737 cuits de personas f√≠sicas que ten√≠an al menos una de deuda en el sistema financiero en Junio de 2019, que se encontraban en situaci√≥n crediticia 1 o 2 (es decir, que no tuvieran atrasos mayores a 90 d√≠as) y cuyo monto total adeudado en ese momento no superaba los 100.000 pesos argentinos. Para los cuits de la muestra aleatoria se registraron y resumieron las deudas en todas las entidades en Junio de 2019 y tambi√©n 6 meses hacia atr√°s. Tambi√©n se registraron las deudas de esos cuits entre Julio 2019 y Junio 2020 para poder evaluar su evoluci√≥n.

Muestra del dataset:

tipo_persona n_deudas_actual proxy_edad_actual deuda_total_actual deuda_con_garantia_actual situacion_mes_actual prop_con_garantia_actual tiene_garantia_actual mora_30_dias_mes_actual n_meses_seg_bcra media_deuda_total media_deuda_situacion_1 media_deuda_situacion_2 media_deuda_con_garantia media_deuda_sin_garantia media_deuda_en_default max_situacion_mes max_sit_mes_con_garantia max_sit_mes_sin_garantia media_prop_situacion_1 media_prop_situacion_2 media_prop_default media_prop_con_garantia prop_tuvo_garantia prop_mora_30_dias_seg prop_default_seg peor_situacion_respuesta default mora_mayor_30_dias 23 1 359 126 126 1 1 1 0 7 156 156 0 153,857142857143 2,14285714285714 0 1 1 1 1 0 0 0,987475247418321 1 0 0 1 0 0 27 1 303 1 0 1 0 0 0 6 2,83333333333333 2,83333333333333 0 0 2,83333333333333 0 1 1 1 0 0 0 0 0 0 1 0 0 27 2 258 14 0 1 0 0 0 7 27,2857142857143 27,2857142857143 0 0 27,2857142857143 0 1 1 1 0 0 0 0 0 0 1 0 0 20 1 274 13 0 2 0 0 1 7 8,85714285714286 7 1,85714285714286 0 8,85714285714286 0 2 2 0,857142857142857 0,142857142857143 0 0 0 0,142857142857143 0 2 0 1 20 2 164 55 0 1 0 0 0 7 67,7142857142857 67,7142857142857 0 0 67,7142857142857 0 1 1 1 0 0 0 0 0 0 1 0 0 27 1 149 24 0 1 0 0 0 7 27,8571428571429 27,8571428571429 0 0 27,8571428571429 0 1 1 1 0 0 0 0 0 0 5 1 1 23 1 174 13 0 1 0 0 0 6 14,8333333333333 14,8333333333333 0 0 14,8333333333333 0 1 1 1 0 0 0 0 0 0 1 0 0 20 2 329 78 0 1 0 0 0 7 54,2857142857143 53,4285714285714 0,857142857142857 0 54,2857142857143 0 2 2 0,976808905380334 0,0231910946196661 0 0 0 0,285714285714286 0 1 0 0 27 3 399 52 0 1 0 0 0 7 42,2857142857143 42,2857142857143 0 0 42,2857142857143 0 1 1 1 0 0 0 0 0 0 1 0 0 20 1 379 8 0 1 0 0 0 7 15,8571428571429 15,8571428571429 0 0 15,8571428571429 0 1 1 1 0 0 0 0 0 0 1 0 0

Descripcion de las variables:

orden, variable, detalle 1, id_individuo, identificaci√≥n del individuo (anonimizada). 2, tipo_persona, primeros dos d√≠gitos del cuit (20: hombres; 27: mujeres). 3, n_deudas_actual, cantidad de entidades en las que el cuit ten√≠a al menos una deuda en Jun-2019. 4, proxy_edad_actual, tres primeros n√∫meros del dni. 5, deuda_total_actual, monto total de deuda en Jun-2019 (expresada en miles de pesos). 6, deuda_con_garantia_actual, monto total de deuda garantizada en Jun-2019 (expresada en miles de pesos). 7, situacion_mes_actual, situaci√≥n crediticia m√°s grave en todas las deudas del cuit en Jun-2019. 8, prop_con_garantia_actual, proporci√≥n de la deuda garantizada en Jun-2019 9, tiene_garantia_actual, variable indicadora (0: no, 1: si) de si el cuit ten√≠a al menos una deuda garantizada en Jun-2019. 10, mora_30_dias_mes_actual, variable indicadora (0: no, 1: si) de si el cuit estaba en situaci√≥n 2 en Jun-2019. 11, n_meses_seg_bcra, cantidad de meses en los que el cuit ten√≠a al menos una deuda informada en el sistema financiero, entre Dic-2018 y Jun-2019. 12, media_deuda_total, promedio de la deuda total entre Dic-2018 y Jun-2019. 13, media_deuda_situacion_1, promedio de la deuda en situaci√≥n 1 entre Dic-2018 y Jun-2019. 14, media_deuda_situacion_2, promedio de la deuda en situaci√≥n 2 entre Dic-2018 y Jun-2019. 15, media_deuda_con_garantia, promedio de la deuda garantizada entre Dic-2018 y Jun-2019. 16, media_deuda_sin_garantia, promedio de la deuda no garantizada entre Dic-2018 y Jun-2019. 17, media_deuda_en_default, promedio de la deuda en defaut (situaci√≥n 3 o peor) entre Dic-2018 y Jun-2019. 18, max_situacion_mes, maxima situaci√≥n entre Dic-2018 y Jun-2019. 19, max_sit_mes_con_garantia, maxima situaci√≥n en las deudas garantizadas entre Dic-2018 y Jun-2019. 20, max_sit_mes_sin_garantia, maxima situaci√≥n en las deudas no garantizadas entre Dic-2018 y Jun-2019. 21, media_prop_situacion_1, promedio de la proporci√≥n de deuda en situaci√≥n 1 entre Dic-2018 y Jun-2019. 22, media_prop_situacion_2, promedio de la proporci√≥n de deuda en situaci√≥n 2 entre Dic-2018 y Jun-2019. 23, media_prop_default, promedio de la proporci√≥n de deuda en default entre Dic-2018 y Jun-2019. 24, media_prop_con_garantia, promedio de la proporci√≥n de deuda garantizada entre Dic-2018 y Jun-2019. 25, prop_tuvo_garantia, proporci√≥n de meses en los cuales el cuit tuvo deuda garantizada, entre Dic-2018 y Jun-2019. 26, prop_mora_30_dias_seg, proporci√≥n de meses en los cuales el cuit estuvo en situaci√≥n 2, entre Dic-2018 y Jun-2019. 27, prop_default_seg, proporci√≥n de meses en los cuales el cuit estuvo en default, entre Dic-2018 y Jun-2019. 28, peor_situacion_respuesta, situaci√≥n crediticia m√°s grave en todas las deudas del cuit entre Jul-2019 y Jun-2020. 29, default, situaci√≥n crediticia m√°s grave mayor o igual 3 en todas las deudas del cuit entre Jul-2019 y Jun-2020. 30, mora_mayor_30_dias, situaci√≥n crediticia m√°s grave igual 2 en todas las deudas del cuit entre Jul-2019 y Jun-2020

Consignas a. Describa la distribuci√≥n univariada de las variables presente en el conjunto de datos. ¬øSe evidencian outliers en alguna de ellas? b. Calcule e interprete la matriz de correlaciones de variables disponibles a Jun-2019 (las posibles predictores de default en el per√≠odo Jul-2019 a Jun-2020). c. Realice un an√°lisis de componentes principales sobre las mismas variables. ¬øQu√© porcentaje de la variabilidad total logran explicar las dos primeras componentes? ¬øEs posible realizar una interpretaci√≥n sobre los componentes? ¬øCu√°l? ¬øLogran esas componentes diferenciar a los cuits seg√∫n el tipo de persona? d. ¬øExisten distintos subgrupos de cuits en los datos? ¬øCu√°ntos logra identificar? ¬øQu√© caracter√≠sticas tienen? Explique la metodolog√≠a utilizada. e. Construya un modelo predictivo para a variable respuesta ‚Äòdefault‚Äô utilizando s√≥lo las variables disponibles a Jun-2019. ¬øQu√© capacidad predictiva tiene ese modelo? f. ¬øUtilizar√≠a el modelo construido para evaluar futuros solicitantes de un cr√©dito? Justifique su respuesta.

Aclaraciones:

Se utilizaran los metodos necesarios: reglas de asociacion, PCA, analisis de clusters, arboles de clasificacion y regresion (cart y/o random forest), siguiendo las pautas de estilo de tidyverse trabajando en R + quarto

Se entiende el problema? Necesitas mas informacion?

###### ChatGPT Plus

| 

S√≠, el problema est√° bien planteado y las consignas son claras. Se realizar√° un an√°lisis detallado de la distribuci√≥n de variables, detecci√≥n de outliers, correlaciones, PCA, clustering y modelado predictivo con CART y/o Random Forest.

Solo unas aclaraciones para confirmar:

1.  ¬øSe requiere un preprocesamiento espec√≠fico, como transformaci√≥n de variables o imputaci√≥n de valores faltantes?

2.  ¬øSe debe comparar diferentes modelos predictivos o solo uno?

3.  ¬øSe usar√° una m√©trica espec√≠fica para evaluar el modelo (AUC, precisi√≥n, F1-score, etc.)?

4.  ¬øEl objetivo final es solo evaluar la capacidad predictiva o tambi√©n proporcionar recomendaciones sobre la concesi√≥n de cr√©ditos?

Si hay alg√∫n otro detalle relevante, av√≠same antes de comenzar.

##### T√∫ dijiste:

1\. si. a la columna max_sit_mes_con_garantia, asignarle 0 si la deuda con garantia actual es 0 y tiene NA 2. si 3. usar al menos 5 metricas para compararlos 4. ambas: evaluar la capacidad predictiva y proporcionar las recomendaciones

###### ChatGPT Plus

| 

Perfecto, con esos puntos en claro, podemos avanzar.

Voy a estructurar el an√°lisis en los siguientes pasos:

1.  **Limpieza y preprocesamiento de datos**

    -   Imputaci√≥n de `max_sit_mes_con_garantia`.

    -   Detecci√≥n y tratamiento de outliers si es necesario.

    -   Transformaciones o normalizaciones necesarias.

2.  **An√°lisis exploratorio de datos (EDA)**

    -   Distribuciones univariadas de las variables.

    -   Detecci√≥n visual y estad√≠stica de outliers.

    -   Correlaciones entre las variables predictoras.

3.  **Reducci√≥n de dimensionalidad (PCA)**

    -   An√°lisis de componentes principales.

    -   Interpretaci√≥n de las principales componentes.

    -   Evaluaci√≥n de si diferencian los tipos de personas.

4.  **Segmentaci√≥n de clientes (Clustering)**

    -   Identificaci√≥n de grupos dentro de los deudores.

    -   Caracterizaci√≥n de los segmentos encontrados.

5.  **Modelado predictivo del default**

    -   Construcci√≥n de al menos dos modelos (CART, Random Forest, otros).

    -   Comparaci√≥n con al menos cinco m√©tricas de desempe√±o.

    -   Evaluaci√≥n de importancia de variables.

6.  **Recomendaciones y conclusiones**

    -   Evaluaci√≥n del uso del modelo para futuros cr√©ditos.

    -   Limitaciones del an√°lisis y posibles mejoras.

Voy a comenzar con la limpieza e imputaci√≥n de datos en R. ¬øQuer√©s que incluya c√≥digo y explicaciones en cada paso o solo el c√≥digo con comentarios?

##### T√∫ dijiste:

si, por favor

###### ChatGPT Plus

Voy a comenzar con el **preprocesamiento de datos**, centr√°ndome en la imputaci√≥n de `max_sit_mes_con_garantia`.

### üìå **Paso 1: Limpieza e imputaci√≥n de datos**

La columna `max_sit_mes_con_garantia` debe tomar el valor **0 si `deuda_con_garantia_actual` es 0 y tiene NA**.

Aqu√≠ est√° el c√≥digo en R con `tidyverse`:

```         
```

```         
```

```         
```

```         
# Cargar paquetes necesarios
library(tidyverse)

# Simulaci√≥n de datos de ejemplo
set.seed(123)
datos <- tibble(
  deuda_con_garantia_actual = c(0, 1, 0, 1, 0, 0, 1, NA, 0, 1),
  max_sit_mes_con_garantia = c(NA, 3, NA, 2, NA, 4, 1, 5, NA, NA)
)

# Aplicar la imputaci√≥n: si deuda_con_garantia_actual es 0 y max_sit_mes_con_garantia es NA, se asigna 0
datos <- datos %>%
  mutate(max_sit_mes_con_garantia = if_else(
    is.na(max_sit_mes_con_garantia) & deuda_con_garantia_actual == 0, 
    0, 
    max_sit_mes_con_garantia
  ))

# Ver resultado
print(datos)
```

üîπ **Explicaci√≥n del c√≥digo:**

-   Se usa `mutate()` para modificar `max_sit_mes_con_garantia`.

-   `if_else()` verifica si el valor es NA y si `deuda_con_garantia_actual` es 0.

-   En esos casos, se reemplaza con **0**, en los dem√°s se mantiene el valor original.

------------------------------------------------------------------------

### üìä **Siguientes pasos**

Si esta imputaci√≥n est√° bien, avanzamos con:

1.  **Detecci√≥n y tratamiento de outliers**

2.  **EDA (An√°lisis Exploratorio de Datos)**

¬øQuer√©s que analicemos outliers en alguna variable en particular?

##### T√∫ dijiste:

analiza outliers en todas las variables que sea necesario y factible hacerlo

###### ChatGPT Plus

Voy a analizar los **outliers** en todas las variables num√©ricas del dataset. Utilizar√© el m√©todo de **caja y bigotes (IQR)** para identificar valores at√≠picos.

------------------------------------------------------------------------

### üìå **Paso 1: Identificar variables num√©ricas**

Usaremos `select(where(is.numeric))` para trabajar solo con las variables num√©ricas.

### üìå **Paso 2: Detecci√≥n de outliers con IQR**

Un valor es considerado **outlier** si est√° fuera del rango:

\[Q1‚àí1.5√óIQR,Q3+1.5√óIQR\]\[Q1‚àí1.5√óIQR,Q3+1.5√óIQR\]

donde:

-   **Q1** es el **percentil 25**

-   **Q3** es el **percentil 75**

-   **IQR** es la **diferencia entre Q3 y Q1**

Aqu√≠ est√° el c√≥digo en R:

------------------------------------------------------------------------

```         
```

```         
```

```         
```

```         
# Cargar paquetes
library(tidyverse)

# Funci√≥n para detectar outliers con IQR
detectar_outliers <- function(df) {
  df %>%
    select(where(is.numeric)) %>%  # Filtrar solo columnas num√©ricas
    pivot_longer(everything(), names_to = "variable", values_to = "valor") %>%
    group_by(variable) %>%
    summarise(
      Q1 = quantile(valor, 0.25, na.rm = TRUE),
      Q3 = quantile(valor, 0.75, na.rm = TRUE),
      IQR = Q3 - Q1,
      Limite_Inferior = Q1 - 1.5 * IQR,
      Limite_Superior = Q3 + 1.5 * IQR,
      Outliers = sum(valor < Limite_Inferior | valor > Limite_Superior, na.rm = TRUE),
      Total_Datos = sum(!is.na(valor)),
      Porcentaje_Outliers = round((Outliers / Total_Datos) * 100, 2)
    ) %>%
    arrange(desc(Porcentaje_Outliers))
}

# Aplicar funci√≥n al dataset
resumen_outliers <- detectar_outliers(datos)

# Mostrar resultado
print(resumen_outliers)
```

------------------------------------------------------------------------

### üìä **Interpretaci√≥n del resultado**

1.  **La tabla muestra**:

    -   La cantidad de outliers por variable

    -   El porcentaje de datos at√≠picos

    -   Los l√≠mites superior e inferior

2.  **Si una variable tiene muchos outliers**, podemos:

    -   **Mantenerlos**, si tienen significado dentro del contexto

    -   **Caparlos (Winsorizing)** si son extremos pero reales

    -   **Eliminar valores err√≥neos**, si se deben a errores de carga

------------------------------------------------------------------------

### üìå **Siguientes pasos**

üîπ **¬øTe gustar√≠a visualizar los outliers con boxplots antes de decidir c√≥mo tratarlos?**

##### T√∫ dijiste:

si, por favor, y agregar el codigo para visualizacion en una matriz de 6 columnas

###### ChatGPT Plus

Voy a generar una **matriz de boxplots** con 6 columnas para visualizar los outliers de las variables num√©ricas.

------------------------------------------------------------------------

### üìå **C√≥digo R para visualizaci√≥n**

```         
```

```         
```

```         
```

```         
# Cargar paquetes
library(tidyverse)
library(ggplot2)

# Funci√≥n para graficar boxplots en una matriz de 6 columnas
graficar_boxplots <- function(df) {
  df %>%
    select(where(is.numeric)) %>%
    pivot_longer(everything(), names_to = "variable", values_to = "valor") %>%
    ggplot(aes(x = variable, y = valor)) +
    geom_boxplot(outlier.color = "red", outlier.shape = 16, outlier.size = 2) +
    facet_wrap(~variable, scales = "free", ncol = 6) +  # Matriz de 6 columnas
    theme_minimal() +
    theme(axis.text.x = element_blank(),
          axis.ticks.x = element_blank(),
          strip.text = element_text(size = 10, face = "bold")) +
    labs(title = "Distribuci√≥n de Outliers por Variable",
         y = "Valor", x = "")
}

# Aplicar la funci√≥n al dataset
graficar_boxplots(datos)
```

------------------------------------------------------------------------

### üìä **Explicaci√≥n del c√≥digo**

1.  **Se seleccionan las variables num√©ricas** con `select(where(is.numeric))`.

2.  **Se reorganizan en formato largo** (`pivot_longer`) para facilitar la visualizaci√≥n.

3.  **Se genera un boxplot por variable** con `geom_boxplot()`, resaltando outliers en rojo.

4.  **Los gr√°ficos se organizan en una matriz de 6 columnas** (`facet_wrap(~variable, ncol = 6)`).

5.  **Se mejora el dise√±o con `theme_minimal()`** y se eliminan etiquetas innecesarias.

------------------------------------------------------------------------

### üìå **¬øC√≥mo interpretar los gr√°ficos?**

-   **Puntos rojos** ‚Üí outliers detectados.

-   **Si hay valores extremadamente alejados**, analizar si deben **tratarse o mantenerse**.

üîπ **¬øQuieres que implemente una estrategia autom√°tica para tratarlos?** (Ejemplo: Winsorizing, eliminaci√≥n, etc.)

##### T√∫ dijiste:

como podriamos manejar los outliers usando clustering?

###### ChatGPT Plus

Podemos manejar los **outliers** usando **clustering** de varias maneras. Un enfoque com√∫n es usar un **modelo de clustering** para detectar observaciones que est√°n **aisladas** o **pertenecen a un peque√±o grupo separado**. Algunas opciones son:

------------------------------------------------------------------------

### üìå **M√©todos de Clustering para Detectar Outliers**

#### **1Ô∏è‚É£ DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**

-   Detecta outliers como **ruido** (puntos que no pertenecen a ning√∫n cl√∫ster denso).

-   Ideal para conjuntos de datos con **formas irregulares**.

-   No requiere especificar el n√∫mero de cl√∫steres.

-   Identifica los puntos alejados del resto como **outliers**.

#### **2Ô∏è‚É£ K-Means + Distancia al Centroide**

-   Se asigna cada observaci√≥n a un **cl√∫ster**.

-   Los puntos con mayor **distancia al centroide** pueden considerarse **outliers**.

-   Se pueden definir **umbrales** para marcar valores an√≥malos.

#### **3Ô∏è‚É£ Isolation Forest o LOF (Local Outlier Factor)**

-   T√©cnicas especializadas en detecci√≥n de anomal√≠as que pueden combinarse con clustering.

-   Isolation Forest a√≠sla anomal√≠as en pocas iteraciones.

-   LOF eval√∫a la densidad local y detecta puntos con densidad baja.

------------------------------------------------------------------------

### üìå **Implementaci√≥n en R con DBSCAN**

```         
```

```         
```

```         
```

```         
# Cargar paquetes
library(dbscan)
library(ggplot2)
library(tidyverse)

# Normalizar las variables num√©ricas
datos_norm <- datos %>%
  select(where(is.numeric)) %>%
  scale()

# Aplicar DBSCAN (par√°metros √≥ptimos deben ajustarse seg√∫n el dataset)
modelo_dbscan <- dbscan(datos_norm, eps = 0.5, minPts = 5)

# Agregar etiquetas de cluster y detectar outliers (-1 en DBSCAN)
datos <- datos %>%
  mutate(cluster = as.factor(modelo_dbscan$cluster),
         outlier = ifelse(cluster == "-1", "S√≠", "No"))

# Visualizaci√≥n: PCA para reducir dimensiones
pca <- prcomp(datos_norm, center = TRUE, scale. = TRUE)
df_pca <- as.data.frame(pca$x[, 1:2]) %>%
  mutate(cluster = datos$cluster, outlier = datos$outlier)

ggplot(df_pca, aes(x = PC1, y = PC2, color = outlier)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Detecci√≥n de Outliers con DBSCAN",
       x = "Componente Principal 1", y = "Componente Principal 2") +
  scale_color_manual(values = c("No" = "blue", "S√≠" = "red")) +
  theme_minimal()
```

------------------------------------------------------------------------

### üìå **Explicaci√≥n**

1.  **Se normalizan las variables num√©ricas** para evitar sesgos por escalas diferentes.

2.  **Se aplica DBSCAN** para identificar outliers (los que no pertenecen a ning√∫n grupo).

3.  **Se usa PCA** para reducir la dimensionalidad y visualizar los resultados.

4.  **Los puntos rojos en el gr√°fico** representan los **outliers detectados**.

------------------------------------------------------------------------

üîπ **¬øQuieres que probemos otro m√©todo como K-Means o Isolation Forest?**

##### T√∫ dijiste:

si, por favor, con ambos

###### ChatGPT Plus

Aqu√≠ tienes la implementaci√≥n de **K-Means con distancia al centroide** e **Isolation Forest** para detectar outliers en tu dataset.

------------------------------------------------------------------------

## üìå **1Ô∏è‚É£ K-Means + Distancia al Centroide**

**Idea:** Los outliers son los puntos m√°s alejados del centroide de su cl√∫ster.

### **C√≥digo en R**

```         
```

```         
```

```         
```

```         
# Cargar paquetes
library(tidyverse)
library(cluster)
library(ggplot2)

# Normalizar las variables num√©ricas
datos_norm <- datos %>%
  select(where(is.numeric)) %>%
  scale()

# Determinar el n√∫mero √≥ptimo de cl√∫steres con el m√©todo del codo
set.seed(123)
wss <- map_dbl(1:10, ~kmeans(datos_norm, centers = .x, nstart = 25)$tot.withinss)

ggplot(data.frame(Clusters = 1:10, WSS = wss), aes(x = Clusters, y = WSS)) +
  geom_line() +
  geom_point() +
  labs(title = "M√©todo del Codo para K-Means", x = "N√∫mero de Cl√∫steres", y = "Suma de Cuadrados")

# Aplicar K-Means con el n√∫mero √≥ptimo de clusters (ajustar seg√∫n gr√°fico)
k <- 4  # Ajustar seg√∫n el codo
modelo_kmeans <- kmeans(datos_norm, centers = k, nstart = 25)

# Calcular distancia al centroide m√°s cercano
distancias <- apply(modelo_kmeans$centers[modelo_kmeans$cluster, ], 1, function(centroide) {
  sqrt(rowSums((datos_norm - centroide)^2))
})

# Identificar outliers como los puntos con distancia m√°s alta (percentil 95)
umbral <- quantile(distancias, 0.95)
datos <- datos %>%
  mutate(cluster_kmeans = as.factor(modelo_kmeans$cluster),
         outlier_kmeans = ifelse(distancias > umbral, "S√≠", "No"))

# Visualizaci√≥n con PCA
pca <- prcomp(datos_norm, center = TRUE, scale. = TRUE)
df_pca <- as.data.frame(pca$x[, 1:2]) %>%
  mutate(cluster = datos$cluster_kmeans, outlier = datos$outlier_kmeans)

ggplot(df_pca, aes(x = PC1, y = PC2, color = outlier)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Detecci√≥n de Outliers con K-Means",
       x = "Componente Principal 1", y = "Componente Principal 2") +
  scale_color_manual(values = c("No" = "blue", "S√≠" = "red")) +
  theme_minimal()
```

------------------------------------------------------------------------

## üìå **2Ô∏è‚É£ Isolation Forest**

**Idea:** Un algoritmo de bosque aleatorio que a√≠sla outliers en menos iteraciones.

### **C√≥digo en R**

```         
```

```         
```

```         
```

```         
# Cargar paquete
library(isotree)

# Entrenar modelo de Isolation Forest
modelo_iforest <- isolation.forest(datos_norm, ntrees = 100)

# Obtener puntajes de anomal√≠a (cuanto mayor, m√°s probable que sea outlier)
scores <- predict(modelo_iforest, datos_norm, type = "score")

# Definir umbral de outlier (percentil 95)
umbral_iforest <- quantile(scores, 0.95)
datos <- datos %>%
  mutate(outlier_iforest = ifelse(scores > umbral_iforest, "S√≠", "No"))

# Visualizaci√≥n con PCA
df_pca <- df_pca %>%
  mutate(outlier_iforest = datos$outlier_iforest)

ggplot(df_pca, aes(x = PC1, y = PC2, color = outlier_iforest)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Detecci√≥n de Outliers con Isolation Forest",
       x = "Componente Principal 1", y = "Componente Principal 2") +
  scale_color_manual(values = c("No" = "blue", "S√≠" = "red")) +
  theme_minimal()
```

------------------------------------------------------------------------

## üìå **Resumen**

| M√©todo | Ventajas | Desventajas |
|------------------------|------------------------|------------------------|
| **DBSCAN** | Detecta outliers sin necesidad de predefinir `k`. Ideal para datos con formas irregulares. | Sensible a `eps` y `minPts`, puede fallar en datos de alta dimensi√≥n. |
| **K-Means + Distancia** | F√°cil de interpretar y visualizar. Funciona bien en datos estructurados. | Necesita definir `k`, no detecta bien outliers en datos no esf√©ricos. |
| **Isolation Forest** | Eficiente en grandes vol√∫menes de datos, no necesita definir `k`. | Puede ser menos interpretable que K-Means. |

üîπ **¬øCu√°l de los m√©todos te resulta m√°s √∫til para tu an√°lisis?** üöÄ

##### T√∫ dijiste:

usar un metodo de aglomeracion jerarquico usando dendogramas e identificar los clusters usando el indice silhouette

###### ChatGPT Plus

Aqu√≠ tienes el an√°lisis de clustering jer√°rquico con dendrogramas e identificaci√≥n de clusters mediante el **√≠ndice de Silhouette** en R.

------------------------------------------------------------------------

## üìå **1Ô∏è‚É£ Clustering Jer√°rquico + Dendrograma**

**Idea:** Agrupamos las observaciones bas√°ndonos en similitud y visualizamos la estructura en un dendrograma.

### **C√≥digo en R**

```         
```

```         
```

```         
```

```         
# Cargar paquetes
library(tidyverse)
library(cluster)
library(factoextra)

# Normalizar los datos num√©ricos
datos_norm <- datos %>%
  select(where(is.numeric)) %>%
  scale()

# Crear una matriz de distancias (m√©todo Euclidiano)
dist_matrix <- dist(datos_norm, method = "euclidean")

# Aplicar clustering jer√°rquico usando el m√©todo de Ward
hclust_model <- hclust(dist_matrix, method = "ward.D2")

# Visualizar el dendrograma
fviz_dend(hclust_model, k = 4, 
          cex = 0.7, 
          rect = TRUE, 
          rect_fill = TRUE, 
          rect_border = "blue", 
          main = "Dendrograma - Clustering Jer√°rquico")
```

------------------------------------------------------------------------

## üìå **2Ô∏è‚É£ Determinaci√≥n del N√∫mero √ìptimo de Clusters con el √çndice de Silhouette**

**Idea:** Se eval√∫a qu√© cantidad de clusters es √≥ptima en base a la separaci√≥n y cohesi√≥n de los grupos.

### **C√≥digo en R**

```         
```

```         
```

```         
```

```         
# Evaluar el √≠ndice de Silhouette para distintos valores de k
silhouette_scores <- map_dbl(2:10, function(k) {
  cluster_assignments <- cutree(hclust_model, k = k)
  mean(silhouette(cluster_assignments, dist_matrix)[, 3])
})

# Visualizar el √≠ndice de Silhouette
sil_df <- tibble(k = 2:10, Silhouette = silhouette_scores)
ggplot(sil_df, aes(x = k, y = Silhouette)) +
  geom_line() +
  geom_point(size = 3, color = "red") +
  labs(title = "√çndice de Silhouette para Diferentes Clusters",
       x = "N√∫mero de Clusters", y = "√çndice de Silhouette") +
  theme_minimal()

# Seleccionar el n√∫mero √≥ptimo de clusters (m√°ximo √≠ndice de Silhouette)
optimo_k <- sil_df$k[which.max(sil_df$Silhouette)]
cat("N√∫mero √≥ptimo de clusters seg√∫n Silhouette:", optimo_k, "\n")

# Asignar clusters a cada observaci√≥n usando el n√∫mero √≥ptimo
datos <- datos %>%
  mutate(cluster_hclust = cutree(hclust_model, k = optimo_k))
```

------------------------------------------------------------------------

## üìå **3Ô∏è‚É£ Visualizaci√≥n de los Clusters con PCA**

```         
```

```         
```

```         
```

```         
# Aplicar PCA para visualizaci√≥n
pca <- prcomp(datos_norm, center = TRUE, scale. = TRUE)
df_pca <- as.data.frame(pca$x[, 1:2]) %>%
  mutate(cluster = as.factor(datos$cluster_hclust))

# Graficar los clusters en el espacio PCA
ggplot(df_pca, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.7, size = 3) +
  labs(title = "Clusters Identificados con Clustering Jer√°rquico",
       x = "Componente Principal 1", y = "Componente Principal 2") +
  theme_minimal()
```

------------------------------------------------------------------------

## üìå **Resumen de Resultados**

1Ô∏è‚É£ Se gener√≥ un dendrograma para visualizar la estructura de los datos.\
2Ô∏è‚É£ Se us√≥ el **√≠ndice de Silhouette** para determinar el n√∫mero √≥ptimo de clusters.\
3Ô∏è‚É£ Se asignaron clusters y se visualiz√≥ la distribuci√≥n en un **gr√°fico PCA**.

üîπ **¬øTe gustar√≠a que analicemos m√°s caracter√≠sticas de los clusters formados?** üöÄ

ChatGPT puede cometer errores. Considera verificar la informaci√≥n importante.
